{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca6b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import csv\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"data/hb_train_1.csv\", header=None)\n",
    "val = pd.read_csv(\"data/hb_val.csv\", header=None)\n",
    "test = pd.read_csv(\"data/hb_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371d55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = train.iloc[:,0]\n",
    "# X_train = train.iloc[:, [1,27]]\n",
    "# y_test = test.iloc[:,0]\n",
    "# X_test = test.iloc[:, [1,27]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c19f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "# param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "# model = xgb.train(param, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd8ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf33900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee61220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the true positive, false positive, true negative, and false negative counts.\n",
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "           TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "           TN += 1\n",
    "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "# perf_measure(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04eba0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "train = loadtxt(\"data/hb_train_1.csv\", delimiter=\",\" )\n",
    "val = loadtxt(\"data/hb_val.csv\", delimiter=\",\" )\n",
    "test = loadtxt(\"data/hb_test.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b98bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[:, 0]\n",
    "X_train = train[:, 1:27]\n",
    "y_test = test[:, 0]\n",
    "X_test = test[:, 1:27]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6590d",
   "metadata": {},
   "source": [
    "## XGBoost on one site's data (61% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85797cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewzhang/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:104: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:15:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.96      0.71     20032\n",
      "         1.0       0.87      0.25      0.39     19968\n",
      "\n",
      "    accuracy                           0.61     40000\n",
      "   macro avg       0.71      0.61      0.55     40000\n",
      "weighted avg       0.71      0.61      0.55     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(use_label_encoder=False)\n",
    "model.fit(X_train, y_train, verbose=2)\n",
    "# make predictions for test data\n",
    "xgb_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b877ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5006, 775, 19257, 14962)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_measure(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81173f70",
   "metadata": {},
   "source": [
    "# 2 sites data regular xgboost (71% accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad3ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewzhang/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:104: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:15:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.71      0.71     20032\n",
      "         1.0       0.71      0.71      0.71     19968\n",
      "\n",
      "    accuracy                           0.71     40000\n",
      "   macro avg       0.71      0.71      0.71     40000\n",
      "weighted avg       0.71      0.71      0.71     40000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5006, 775, 19257, 14962)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input2 = loadtxt(\"data/hb_train_2.csv\", delimiter=\",\" )\n",
    "\n",
    "train2 = np.concatenate([train, input2])\n",
    "\n",
    "yt2 = train2[:, 0]\n",
    "xt2 = train2[:, 1:27]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False)\n",
    "model.fit(xt2, yt2)\n",
    "# make predictions for test data\n",
    "xgb_pred2 = model.predict(X_test)\n",
    "print(classification_report(y_test, xgb_pred2))\n",
    "perf_measure(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20046c95",
   "metadata": {},
   "source": [
    "# Federated: Voting between two sites (70% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a942e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewzhang/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:104: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:16:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.96      0.71     20032\n",
      "         1.0       0.87      0.25      0.39     19968\n",
      "\n",
      "    accuracy                           0.61     40000\n",
      "   macro avg       0.71      0.61      0.55     40000\n",
      "weighted avg       0.71      0.61      0.55     40000\n",
      "\n",
      "(5006, 775, 19257, 14962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewzhang/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:104: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:16:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.25      0.38     20032\n",
      "         1.0       0.56      0.96      0.71     19968\n",
      "\n",
      "    accuracy                           0.60     40000\n",
      "   macro avg       0.72      0.60      0.55     40000\n",
      "weighted avg       0.72      0.60      0.55     40000\n",
      "\n",
      "(19244, 15106, 4926, 724)\n"
     ]
    }
   ],
   "source": [
    "model1 = XGBClassifier(use_label_encoder=False)\n",
    "model1.fit(X_train, y_train)\n",
    "pred1 = model1.predict(X_test)\n",
    "print(classification_report(y_test, pred1))\n",
    "print(perf_measure(y_test, pred1))\n",
    "\n",
    "data2 = loadtxt(\"data/hb_train_2.csv\", delimiter=\",\" )\n",
    "xt2 = data2[:, 1:27]\n",
    "yt2 = data2[:, 0]\n",
    "model2 = XGBClassifier(use_label_encoder=False)\n",
    "model2.fit(xt2, yt2)\n",
    "pred2 = model2.predict(X_test)\n",
    "print(classification_report(y_test, pred2))\n",
    "print(perf_measure(y_test, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6060f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9758602 , 0.02413985],\n",
       "       [0.8741944 , 0.1258056 ],\n",
       "       [0.96343887, 0.0365611 ],\n",
       "       ...,\n",
       "       [0.20763099, 0.792369  ],\n",
       "       [0.69789803, 0.302102  ],\n",
       "       [0.62504923, 0.3749508 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = model1.predict_proba(X_test)\n",
    "prob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ec2b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9060826 , 0.09391744],\n",
       "       [0.3606001 , 0.6393999 ],\n",
       "       [0.591596  , 0.408404  ],\n",
       "       ...,\n",
       "       [0.05627292, 0.9437271 ],\n",
       "       [0.06083065, 0.93916935],\n",
       "       [0.1133765 , 0.8866235 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2 = model2.predict_proba(X_test)\n",
    "prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feb3442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.71      0.70     20032\n",
      "         1.0       0.70      0.70      0.70     19968\n",
      "\n",
      "    accuracy                           0.70     40000\n",
      "   macro avg       0.70      0.70      0.70     40000\n",
      "weighted avg       0.70      0.70      0.70     40000\n",
      "\n",
      "TP, FP, TN, FN: \n",
      "(13934, 5898, 14134, 6034)\n"
     ]
    }
   ],
   "source": [
    "average = prob1 / 2 + prob2 / 2\n",
    "avg_pred = []\n",
    "for ar in average:\n",
    "    if ar[0] > ar[1]:\n",
    "        avg_pred.append(0)\n",
    "    else:\n",
    "        avg_pred.append(1)\n",
    "print(classification_report(y_test, avg_pred))\n",
    "print(\"TP, FP, TN, FN: \")\n",
    "print(perf_measure(y_test, avg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356dec88",
   "metadata": {},
   "source": [
    "# Federated Voting between 2 sites (logistic reg voting) The log reg part trained with central site data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a4a5b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9060826 , 0.09391744],\n",
       "       [0.3606001 , 0.6393999 ],\n",
       "       [0.591596  , 0.408404  ],\n",
       "       ...,\n",
       "       [0.05627292, 0.9437271 ],\n",
       "       [0.06083065, 0.93916935],\n",
       "       [0.1133765 , 0.8866235 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d0adfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9758602 , 0.02413985],\n",
       "       [0.8741944 , 0.1258056 ],\n",
       "       [0.96343887, 0.0365611 ],\n",
       "       ...,\n",
       "       [0.20763099, 0.792369  ],\n",
       "       [0.69789803, 0.302102  ],\n",
       "       [0.62504923, 0.3749508 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "105b6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_test = []\n",
    "for i in range(len(prob1)):\n",
    "    prob_test.append([prob1[i][1], prob2[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "442c62c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.15700218, 0.55877423],\n",
       " [0.3182348, 0.93387353],\n",
       " [0.25341344, 0.90275633],\n",
       " [0.06046845, 0.6857839],\n",
       " [0.27123946, 0.90233976],\n",
       " [0.43525603, 0.68828034],\n",
       " [0.19647233, 0.9031749],\n",
       " [0.33865198, 0.95997345],\n",
       " [0.13553736, 0.6774776],\n",
       " [0.07931057, 0.5396606],\n",
       " [0.64061594, 0.97943485],\n",
       " [0.3930463, 0.87617826],\n",
       " [0.05147813, 0.6556781],\n",
       " [0.18030292, 0.8899746],\n",
       " [0.20751336, 0.7927603],\n",
       " [0.62632674, 0.96337897],\n",
       " [0.1129857, 0.9016411],\n",
       " [0.18827327, 0.89116734],\n",
       " [0.319663, 0.87768227],\n",
       " [0.1768981, 0.88257706],\n",
       " [0.015323583, 0.32452908],\n",
       " [0.0053059654, 0.103031166],\n",
       " [0.07234937, 0.59910154],\n",
       " [0.12378976, 0.7605449],\n",
       " [0.045408737, 0.6926247],\n",
       " [0.38030472, 0.6160172],\n",
       " [0.09111402, 0.40672454],\n",
       " [0.07827512, 0.5556533],\n",
       " [0.05799102, 0.56407493],\n",
       " [0.10492211, 0.7245076],\n",
       " [0.14970513, 0.6064512],\n",
       " [0.08844881, 0.4903831],\n",
       " [0.099104464, 0.6201483],\n",
       " [0.11500091, 0.79405165],\n",
       " [0.12764533, 0.61677593],\n",
       " [0.28474987, 0.94610596],\n",
       " [0.0072812526, 0.47635084],\n",
       " [0.13155876, 0.85462016],\n",
       " [0.06976031, 0.7854142],\n",
       " [0.07118247, 0.6445087],\n",
       " [0.027677476, 0.7809005],\n",
       " [0.41598663, 0.956201],\n",
       " [0.032369602, 0.23302968],\n",
       " [0.20349835, 0.76827866],\n",
       " [0.02097257, 0.695794],\n",
       " [0.21288542, 0.8852968],\n",
       " [0.14750643, 0.62522596],\n",
       " [0.18787752, 0.84552395],\n",
       " [0.13319653, 0.38204983],\n",
       " [0.07123697, 0.67920476],\n",
       " [0.15949053, 0.67574793],\n",
       " [0.1329393, 0.8566943],\n",
       " [0.27008492, 0.8676427],\n",
       " [0.24129075, 0.8859129],\n",
       " [0.016241873, 0.15948068],\n",
       " [0.12862119, 0.86261374],\n",
       " [0.124478914, 0.8807318],\n",
       " [0.25055215, 0.91549486],\n",
       " [0.16717729, 0.8399285],\n",
       " [0.04978052, 0.46535662],\n",
       " [0.14288709, 0.79318315],\n",
       " [0.0580731, 0.81124204],\n",
       " [0.06200673, 0.6015896],\n",
       " [0.021113303, 0.5337952],\n",
       " [0.053633045, 0.80405515],\n",
       " [0.23084077, 0.9292451],\n",
       " [0.2860116, 0.920547],\n",
       " [0.16282104, 0.9036978],\n",
       " [0.066206805, 0.46885943],\n",
       " [0.012771086, 0.12376039],\n",
       " [0.14394988, 0.8509067],\n",
       " [0.11248252, 0.8495867],\n",
       " [0.06274212, 0.6930346],\n",
       " [0.11965475, 0.6703119],\n",
       " [0.3939345, 0.72660065],\n",
       " [0.03061771, 0.31561404],\n",
       " [0.05988863, 0.71683836],\n",
       " [0.24881162, 0.95218986],\n",
       " [0.10968312, 0.6783247],\n",
       " [0.050523467, 0.3228791],\n",
       " [0.052153606, 0.68683386],\n",
       " [0.33318946, 0.7042714],\n",
       " [0.29854715, 0.8936842],\n",
       " [0.034194354, 0.7112123],\n",
       " [0.70797527, 0.9543505],\n",
       " [0.17961952, 0.7593372],\n",
       " [0.048690066, 0.80400264],\n",
       " [0.6821497, 0.9474689],\n",
       " [0.014491664, 0.12972552],\n",
       " [0.034740876, 0.6139445],\n",
       " [0.15844624, 0.7584077],\n",
       " [0.2113258, 0.8290767],\n",
       " [0.09454018, 0.45727155],\n",
       " [0.019794086, 0.15201029],\n",
       " [0.09322102, 0.59687346],\n",
       " [0.21522, 0.9715002],\n",
       " [0.2794791, 0.8796483],\n",
       " [0.06761932, 0.6534628],\n",
       " [0.22728762, 0.7998111],\n",
       " [0.016300682, 0.22858946],\n",
       " [0.2142133, 0.78095925],\n",
       " [0.32911956, 0.8282548],\n",
       " [0.11659081, 0.6224105],\n",
       " [0.04222101, 0.57412887],\n",
       " [0.113623284, 0.803201],\n",
       " [0.17817256, 0.63664186],\n",
       " [0.78059375, 0.76888573],\n",
       " [0.36931106, 0.90590954],\n",
       " [0.012441883, 0.61345434],\n",
       " [0.18312255, 0.78903455],\n",
       " [0.013190215, 0.29389325],\n",
       " [0.026440177, 0.2763195],\n",
       " [0.10822731, 0.53397274],\n",
       " [0.37371597, 0.9228413],\n",
       " [0.065525636, 0.7186066],\n",
       " [0.055097565, 0.14739764],\n",
       " [0.12783733, 0.85818094],\n",
       " [0.04545621, 0.5807596],\n",
       " [0.15483572, 0.686148],\n",
       " [0.0638301, 0.66274005],\n",
       " [0.02673589, 0.5449347],\n",
       " [0.01787982, 0.5788276],\n",
       " [0.27344394, 0.56740075],\n",
       " [0.0308525, 0.44234776],\n",
       " [0.06218313, 0.73226815],\n",
       " [0.12324063, 0.67847997],\n",
       " [0.33778396, 0.9030375],\n",
       " [0.28525603, 0.9359904],\n",
       " [0.45005783, 0.94667226],\n",
       " [0.3275455, 0.94898224],\n",
       " [0.17611451, 0.9092661],\n",
       " [0.09846312, 0.6576748],\n",
       " [0.1303116, 0.6774182],\n",
       " [0.57848614, 0.95941734],\n",
       " [0.032035355, 0.19595528],\n",
       " [0.031523988, 0.76111877],\n",
       " [0.30784664, 0.8386965],\n",
       " [0.09705284, 0.68398297],\n",
       " [0.17004457, 0.91751224],\n",
       " [0.04208663, 0.3609109],\n",
       " [0.03588554, 0.4318332],\n",
       " [0.05609277, 0.4976648],\n",
       " [0.09615798, 0.7655214],\n",
       " [0.58409965, 0.8745711],\n",
       " [0.017801646, 0.24461468],\n",
       " [0.07196529, 0.6533073],\n",
       " [0.21989465, 0.6969721],\n",
       " [0.16467313, 0.74084455],\n",
       " [0.050771426, 0.3098078],\n",
       " [0.09935798, 0.5768226],\n",
       " [0.13787375, 0.44748282],\n",
       " [0.4947852, 0.9666039],\n",
       " [0.05160131, 0.66111857],\n",
       " [0.09174007, 0.7340284],\n",
       " [0.0084485905, 0.052811924],\n",
       " [0.17447656, 0.74783754],\n",
       " [0.05583444, 0.4937532],\n",
       " [0.30170316, 0.91864634],\n",
       " [0.041546762, 0.22559142],\n",
       " [0.06559042, 0.82304305],\n",
       " [0.2013588, 0.7508272],\n",
       " [0.09271397, 0.75001204],\n",
       " [0.09868873, 0.60340095],\n",
       " [0.14986497, 0.6518416],\n",
       " [0.49956718, 0.95911777],\n",
       " [0.1459662, 0.5738803],\n",
       " [0.32280013, 0.90008414],\n",
       " [0.40298542, 0.89256597],\n",
       " [0.23201752, 0.68904567],\n",
       " [0.26751673, 0.8228811],\n",
       " [0.014972494, 0.32439598],\n",
       " [0.1348602, 0.8685288],\n",
       " [0.088735044, 0.2935084],\n",
       " [0.13541317, 0.7575482],\n",
       " [0.43469656, 0.9370366],\n",
       " [0.3513922, 0.93865967],\n",
       " [0.61605024, 0.9001221],\n",
       " [0.3232865, 0.8003713],\n",
       " [0.24599816, 0.7209662],\n",
       " [0.16362742, 0.84124064],\n",
       " [0.25816795, 0.79752463],\n",
       " [0.7839015, 0.86843175],\n",
       " [0.5719896, 0.9762012],\n",
       " [0.08711681, 0.82056147],\n",
       " [0.03715316, 0.64012045],\n",
       " [0.06477752, 0.6390873],\n",
       " [0.13336836, 0.6679157],\n",
       " [0.022128848, 0.6286655],\n",
       " [0.048525453, 0.59096247],\n",
       " [0.7814176, 0.97912955],\n",
       " [0.10892834, 0.86079955],\n",
       " [0.1794766, 0.6358465],\n",
       " [0.034122247, 0.21120766],\n",
       " [0.15275136, 0.8265914],\n",
       " [0.25963348, 0.92784256],\n",
       " [0.063894495, 0.57625794],\n",
       " [0.56893814, 0.93390495],\n",
       " [0.34735724, 0.90601707],\n",
       " [0.11469075, 0.53762275],\n",
       " [0.22087541, 0.8515313],\n",
       " [0.11028536, 0.87724245],\n",
       " [0.04766069, 0.33740142],\n",
       " [0.12404371, 0.82789546],\n",
       " [0.74017763, 0.93219125],\n",
       " [0.40339187, 0.8873856],\n",
       " [0.113832965, 0.4599416],\n",
       " [0.4602018, 0.9177199],\n",
       " [0.5177629, 0.97446454],\n",
       " [0.41083917, 0.8811135],\n",
       " [0.25191808, 0.88342017],\n",
       " [0.013957206, 0.30808493],\n",
       " [0.46661943, 0.9380966],\n",
       " [0.035683118, 0.34397388],\n",
       " [0.025491366, 0.6400625],\n",
       " [0.2419921, 0.7591666],\n",
       " [0.057227764, 0.48347542],\n",
       " [0.2188618, 0.7925647],\n",
       " [0.008778902, 0.15138476],\n",
       " [0.4260001, 0.9112607],\n",
       " [0.13581571, 0.86091185],\n",
       " [0.0141078755, 0.37283662],\n",
       " [0.41177213, 0.91116494],\n",
       " [0.08622438, 0.73501945],\n",
       " [0.040824294, 0.69010055],\n",
       " [0.07334717, 0.5292757],\n",
       " [0.537743, 0.91391885],\n",
       " [0.3408441, 0.8840322],\n",
       " [0.25602224, 0.91392356],\n",
       " [0.052363217, 0.3786615],\n",
       " [0.14756674, 0.848362],\n",
       " [0.03063253, 0.12858282],\n",
       " [0.010650692, 0.13417463],\n",
       " [0.09971761, 0.7238717],\n",
       " [0.44967398, 0.9086329],\n",
       " [0.07184006, 0.7440811],\n",
       " [0.11543069, 0.6415115],\n",
       " [0.17639284, 0.7856518],\n",
       " [0.101911016, 0.6154863],\n",
       " [0.016709615, 0.29773116],\n",
       " [0.17632906, 0.76311994],\n",
       " [0.028937707, 0.31239682],\n",
       " [0.22558224, 0.8575451],\n",
       " [0.06664296, 0.8894316],\n",
       " [0.016979404, 0.58106494],\n",
       " [0.022707166, 0.10433629],\n",
       " [0.2127039, 0.9345633],\n",
       " [0.116021514, 0.69061786],\n",
       " [0.031595986, 0.71689093],\n",
       " [0.17318612, 0.75426],\n",
       " [0.13754465, 0.7917681],\n",
       " [0.074428596, 0.48688185],\n",
       " [0.079775326, 0.60936415],\n",
       " [0.57122177, 0.6853197],\n",
       " [0.056024887, 0.7281446],\n",
       " [0.104768634, 0.6131161],\n",
       " [0.55194306, 0.8921376],\n",
       " [0.17993456, 0.72774136],\n",
       " [0.13254699, 0.62194735],\n",
       " [0.1821818, 0.85184],\n",
       " [0.28254893, 0.7813335],\n",
       " [0.013112148, 0.36647332],\n",
       " [0.08978944, 0.8449995],\n",
       " [0.11131754, 0.81018275],\n",
       " [0.16911907, 0.8944032],\n",
       " [0.21444005, 0.79738116],\n",
       " [0.0552535, 0.8344941],\n",
       " [0.042690217, 0.5189678],\n",
       " [0.07705482, 0.47255152],\n",
       " [0.85066134, 0.983915],\n",
       " [0.15793993, 0.69848686],\n",
       " [0.056720424, 0.29455262],\n",
       " [0.09189553, 0.93819153],\n",
       " [0.2854934, 0.9271138],\n",
       " [0.2193736, 0.87771755],\n",
       " [0.34210938, 0.93117094],\n",
       " [0.20327713, 0.8507497],\n",
       " [0.20338078, 0.74991894],\n",
       " [0.46970457, 0.54181796],\n",
       " [0.017392052, 0.22532223],\n",
       " [0.3446448, 0.92168194],\n",
       " [0.040798094, 0.048125397],\n",
       " [0.24600063, 0.75849855],\n",
       " [0.02434846, 0.653575],\n",
       " [0.035549343, 0.23148559],\n",
       " [0.01611124, 0.31895936],\n",
       " [0.086312644, 0.39781702],\n",
       " [0.11246235, 0.6355635],\n",
       " [0.050041955, 0.51438284],\n",
       " [0.20493169, 0.55134517],\n",
       " [0.39304286, 0.8587336],\n",
       " [0.06853871, 0.5605112],\n",
       " [0.05793157, 0.67703575],\n",
       " [0.19955614, 0.68183666],\n",
       " [0.81586707, 0.96427387],\n",
       " [0.176495, 0.5369889],\n",
       " [0.01567162, 0.35042858],\n",
       " [0.52851397, 0.8703545],\n",
       " [0.24817298, 0.8975901],\n",
       " [0.11190041, 0.7166673],\n",
       " [0.32693806, 0.88076794],\n",
       " [0.5867193, 0.94823986],\n",
       " [0.58239657, 0.9300786],\n",
       " [0.14952597, 0.7302656],\n",
       " [0.117615625, 0.7364586],\n",
       " [0.031335384, 0.15521273],\n",
       " [0.104417086, 0.68803227],\n",
       " [0.07619456, 0.61575156],\n",
       " [0.43156993, 0.93251944],\n",
       " [0.28921953, 0.6981243],\n",
       " [0.1570124, 0.86714256],\n",
       " [0.04694743, 0.48947027],\n",
       " [0.35465738, 0.93922675],\n",
       " [0.064036794, 0.7362265],\n",
       " [0.033405297, 0.24372809],\n",
       " [0.058233496, 0.4885801],\n",
       " [0.031305958, 0.42952737],\n",
       " [0.09604018, 0.679985],\n",
       " [0.069165826, 0.77200997],\n",
       " [0.15728846, 0.39594966],\n",
       " [0.84382004, 0.9154622],\n",
       " [0.02542046, 0.49974978],\n",
       " [0.8530278, 0.9166956],\n",
       " [0.13999966, 0.816927],\n",
       " [0.11038694, 0.6765651],\n",
       " [0.039180554, 0.35996467],\n",
       " [0.045770463, 0.39511794],\n",
       " [0.16488488, 0.79783213],\n",
       " [0.050008517, 0.41269326],\n",
       " [0.7829898, 0.9825776],\n",
       " [0.08811756, 0.5828529],\n",
       " [0.15609168, 0.86212903],\n",
       " [0.35571554, 0.94385964],\n",
       " [0.10055451, 0.057656866],\n",
       " [0.5454113, 0.9485021],\n",
       " [0.12735231, 0.7105419],\n",
       " [0.08526395, 0.6035325],\n",
       " [0.035530906, 0.60978925],\n",
       " [0.01275272, 0.75572693],\n",
       " [0.6048159, 0.94958454],\n",
       " [0.59502935, 0.94940627],\n",
       " [0.4296068, 0.86740535],\n",
       " [0.124648325, 0.82339466],\n",
       " [0.03614247, 0.4915627],\n",
       " [0.17997971, 0.5996531],\n",
       " [0.112517595, 0.5995734],\n",
       " [0.19308785, 0.75299364],\n",
       " [0.032325376, 0.48718005],\n",
       " [0.3577904, 0.8846714],\n",
       " [0.54819494, 0.93765044],\n",
       " [0.23059604, 0.67887956],\n",
       " [0.1432167, 0.561305],\n",
       " [0.20022789, 0.7695962],\n",
       " [0.08884801, 0.719707],\n",
       " [0.027762275, 0.5126423],\n",
       " [0.07405081, 0.47903],\n",
       " [0.12665159, 0.89608294],\n",
       " [0.20460239, 0.90474147],\n",
       " [0.05148387, 0.6571243],\n",
       " [0.088221915, 0.48756072],\n",
       " [0.08395867, 0.69133574],\n",
       " [0.09215788, 0.7022407],\n",
       " [0.26207116, 0.8448276],\n",
       " [0.07577591, 0.17869078],\n",
       " [0.2174642, 0.9005944],\n",
       " [0.16438739, 0.5185102],\n",
       " [0.288073, 0.86892426],\n",
       " [0.026371986, 0.07986592],\n",
       " [0.3331618, 0.72718495],\n",
       " [0.09350202, 0.73130447],\n",
       " [0.21829966, 0.8530017],\n",
       " [0.045669395, 0.4009079],\n",
       " [0.14410515, 0.8698396],\n",
       " [0.24799965, 0.79931486],\n",
       " [0.16163428, 0.88178974],\n",
       " [0.28752154, 0.57049096],\n",
       " [0.14934088, 0.7344846],\n",
       " [0.3491883, 0.9592961],\n",
       " [0.594772, 0.9517952],\n",
       " [0.24811862, 0.820681],\n",
       " [0.41096702, 0.890051],\n",
       " [0.56192964, 0.97532594],\n",
       " [0.050851643, 0.57837224],\n",
       " [0.057684395, 0.5792429],\n",
       " [0.48050818, 0.77739996],\n",
       " [0.12293107, 0.75060993],\n",
       " [0.100382544, 0.52652663],\n",
       " [0.2851116, 0.9391415],\n",
       " [0.32144567, 0.7878876],\n",
       " [0.10769217, 0.69155014],\n",
       " [0.071746506, 0.7206023],\n",
       " [0.4405791, 0.9067444],\n",
       " [0.36263365, 0.84481966],\n",
       " [0.025003687, 0.44660202],\n",
       " [0.6280555, 0.9204227],\n",
       " [0.35757732, 0.79603493],\n",
       " [0.1773225, 0.866873],\n",
       " [0.05990035, 0.60165554],\n",
       " [0.025078075, 0.4755735],\n",
       " [0.076488495, 0.25568292],\n",
       " [0.020165807, 0.27646804],\n",
       " [0.05002559, 0.392274],\n",
       " [0.08337838, 0.7613914],\n",
       " [0.21581577, 0.87554955],\n",
       " [0.27733496, 0.7661788],\n",
       " [0.053909805, 0.58695686],\n",
       " [0.09014478, 0.62223166],\n",
       " [0.15840228, 0.8807165],\n",
       " [0.024268249, 0.6069231],\n",
       " [0.18817204, 0.8112279],\n",
       " [0.087318935, 0.5347336],\n",
       " [0.11977972, 0.7807916],\n",
       " [0.04297166, 0.3376862],\n",
       " [0.083428666, 0.6339392],\n",
       " [0.35499483, 0.8758738],\n",
       " [0.08394555, 0.63107085],\n",
       " [0.69431055, 0.95732343],\n",
       " [0.64752305, 0.9446603],\n",
       " [0.44685236, 0.89808714],\n",
       " [0.24348207, 0.8792192],\n",
       " [0.16141672, 0.6723256],\n",
       " [0.22906156, 0.8596611],\n",
       " [0.36079934, 0.8588314],\n",
       " [0.11630484, 0.6601874],\n",
       " [0.16044506, 0.85127306],\n",
       " [0.30837408, 0.9779123],\n",
       " [0.6383239, 0.9403211],\n",
       " [0.89227223, 0.98241353],\n",
       " [0.30235675, 0.85707265],\n",
       " [0.23077455, 0.95251834],\n",
       " [0.25845286, 0.8057113],\n",
       " [0.03352061, 0.8955622],\n",
       " [0.20056663, 0.889529],\n",
       " [0.48566747, 0.7264103],\n",
       " [0.24706681, 0.85921633],\n",
       " [0.36282235, 0.9474891],\n",
       " [0.036290422, 0.14836906],\n",
       " [0.4054028, 0.97831035],\n",
       " [0.07707149, 0.6746661],\n",
       " [0.036562044, 0.7189304],\n",
       " [0.20369285, 0.79311264],\n",
       " [0.02478734, 0.12340965],\n",
       " [0.2268794, 0.9188899],\n",
       " [0.078285664, 0.7188628],\n",
       " [0.16061209, 0.8224183],\n",
       " [0.12188523, 0.89123136],\n",
       " [0.13887766, 0.816716],\n",
       " [0.3866393, 0.88927025],\n",
       " [0.14057288, 0.6975746],\n",
       " [0.54554456, 0.9734991],\n",
       " [0.18555367, 0.89542055],\n",
       " [0.06414347, 0.13579106],\n",
       " [0.0835956, 0.64569724],\n",
       " [0.025732163, 0.5919801],\n",
       " [0.458257, 0.95606095],\n",
       " [0.030106308, 0.21247678],\n",
       " [0.066236466, 0.6176247],\n",
       " [0.04294323, 0.41182756],\n",
       " [0.19204196, 0.8427577],\n",
       " [0.06839375, 0.42425287],\n",
       " [0.17751679, 0.7905191],\n",
       " [0.07477456, 0.38914645],\n",
       " [0.06957817, 0.37735766],\n",
       " [0.41442022, 0.83751464],\n",
       " [0.095090695, 0.84796154],\n",
       " [0.23636343, 0.89517254],\n",
       " [0.12800163, 0.7151958],\n",
       " [0.18109669, 0.73998326],\n",
       " [0.14225757, 0.8162882],\n",
       " [0.1197584, 0.6353539],\n",
       " [0.105223894, 0.69736356],\n",
       " [0.015327384, 0.079205245],\n",
       " [0.026532453, 0.3851995],\n",
       " [0.57222486, 0.96962637],\n",
       " [0.009141316, 0.26406172],\n",
       " [0.27397597, 0.7166927],\n",
       " [0.10992842, 0.7138491],\n",
       " [0.028364154, 0.51269674],\n",
       " [0.07211943, 0.60517776],\n",
       " [0.17331487, 0.7719495],\n",
       " [0.02152487, 0.3418548],\n",
       " [0.10866199, 0.7386286],\n",
       " [0.13889256, 0.5879494],\n",
       " [0.094500996, 0.7842783],\n",
       " [0.31242877, 0.79039097],\n",
       " [0.0018053848, 0.20873086],\n",
       " [0.19700295, 0.78586173],\n",
       " [0.21771868, 0.82637346],\n",
       " [0.26389188, 0.8464002],\n",
       " [0.12508793, 0.89086384],\n",
       " [0.54347026, 0.85834867],\n",
       " [0.34064555, 0.8435558],\n",
       " [0.53383577, 0.95885676],\n",
       " [0.11844706, 0.88385177],\n",
       " [0.07203865, 0.32018796],\n",
       " [0.040311594, 0.7877166],\n",
       " [0.06780957, 0.3983866],\n",
       " [0.38121885, 0.87128025],\n",
       " [0.13856734, 0.6598254],\n",
       " [0.12826167, 0.59275776],\n",
       " [0.053656574, 0.6807188],\n",
       " [0.5422768, 0.96560675],\n",
       " [0.072326206, 0.45346823],\n",
       " [0.27472788, 0.9003251],\n",
       " [0.05194789, 0.3555446],\n",
       " [0.4036504, 0.85465544],\n",
       " [0.016527317, 0.8299148],\n",
       " [0.09620049, 0.5950479],\n",
       " [0.01871044, 0.52186304],\n",
       " [0.24406935, 0.88853335],\n",
       " [0.2652729, 0.9394933],\n",
       " [0.05513509, 0.6140976],\n",
       " [0.09995559, 0.92799276],\n",
       " [0.06781067, 0.6613959],\n",
       " [0.021312067, 0.16845682],\n",
       " [0.12942773, 0.8118563],\n",
       " [0.031703815, 0.30796596],\n",
       " [0.13175635, 0.78454936],\n",
       " [0.07210641, 0.5359279],\n",
       " [0.20263255, 0.9235937],\n",
       " [0.045353994, 0.44643745],\n",
       " [0.5227543, 0.9312194],\n",
       " [0.21964867, 0.85975087],\n",
       " [0.5230873, 0.9534405],\n",
       " [0.116228305, 0.86834764],\n",
       " [0.0051863412, 0.15000038],\n",
       " [0.07866211, 0.76783174],\n",
       " [0.13214849, 0.6946458],\n",
       " [0.054505244, 0.6020093],\n",
       " [0.1258022, 0.9139111],\n",
       " [0.96900594, 0.986392],\n",
       " [0.11971557, 0.6592676],\n",
       " [0.012425212, 0.6465942],\n",
       " [0.07968256, 0.54541993],\n",
       " [0.11039574, 0.8298724],\n",
       " [0.13104166, 0.7459186],\n",
       " [0.24674618, 0.9233518],\n",
       " [0.112782076, 0.65161383],\n",
       " [0.14966422, 0.928293],\n",
       " [0.46827456, 0.9442225],\n",
       " [0.6865656, 0.9403063],\n",
       " [0.17077498, 0.7634332],\n",
       " [0.1373656, 0.8878779],\n",
       " [0.032611497, 0.49657714],\n",
       " [0.014957571, 0.37575275],\n",
       " [0.24964353, 0.9031115],\n",
       " [0.83718777, 0.95392376],\n",
       " [0.1993094, 0.909686],\n",
       " [0.16230306, 0.7707281],\n",
       " [0.8478684, 0.9916283],\n",
       " [0.3865933, 0.9485208],\n",
       " [0.55473614, 0.9634666],\n",
       " [0.34083357, 0.93829787],\n",
       " [0.3202762, 0.8793811],\n",
       " [0.25809917, 0.8323709],\n",
       " [0.68441534, 0.94320524],\n",
       " [0.18656711, 0.716435],\n",
       " [0.30692387, 0.96209013],\n",
       " [0.50522596, 0.8054018],\n",
       " [0.054733194, 0.4882565],\n",
       " [0.24878913, 0.8178824],\n",
       " [0.028214855, 0.36603287],\n",
       " [0.22025475, 0.6310956],\n",
       " [0.09416273, 0.6762257],\n",
       " [0.6502175, 0.9621061],\n",
       " [0.21920392, 0.93319756],\n",
       " [0.22389229, 0.84497607],\n",
       " [0.44257867, 0.91647613],\n",
       " [0.066174276, 0.7639012],\n",
       " [0.2239282, 0.7526858],\n",
       " [0.045808602, 0.5044683],\n",
       " [0.4378313, 0.8958778],\n",
       " [0.055823542, 0.8001821],\n",
       " [0.014769989, 0.37570268],\n",
       " [0.52844846, 0.9456768],\n",
       " [0.18823259, 0.94178265],\n",
       " [0.14429878, 0.8844999],\n",
       " [0.13932757, 0.8753344],\n",
       " [0.029090554, 0.7572762],\n",
       " [0.093100615, 0.6677534],\n",
       " [0.57048434, 0.9497552],\n",
       " [0.00060641835, 0.11532065],\n",
       " [0.1848426, 0.8367184],\n",
       " [0.19397458, 0.90294963],\n",
       " [0.07636331, 0.8932133],\n",
       " [0.034054626, 0.22719678],\n",
       " [0.2841596, 0.43990645],\n",
       " [0.016449388, 0.21773754],\n",
       " [0.5237368, 0.94597656],\n",
       " [0.23069043, 0.7239004],\n",
       " [0.34776354, 0.76525736],\n",
       " [0.39223352, 0.9528114],\n",
       " [0.24330665, 0.91007805],\n",
       " [0.07577015, 0.4627308],\n",
       " [0.05161957, 0.6044143],\n",
       " [0.3872781, 0.7007343],\n",
       " [0.45472807, 0.9144807],\n",
       " [0.12402542, 0.8615841],\n",
       " [0.6597653, 0.95134884],\n",
       " [0.036151264, 0.5018079],\n",
       " [0.10875053, 0.75295496],\n",
       " [0.120091826, 0.879571],\n",
       " [0.3021671, 0.8938341],\n",
       " [0.7511827, 0.95295244],\n",
       " [0.24847972, 0.80460614],\n",
       " [0.47345486, 0.9249053],\n",
       " [0.09188768, 0.7726286],\n",
       " [0.018172303, 0.40060776],\n",
       " [0.03255815, 0.63212025],\n",
       " [0.046323605, 0.27445045],\n",
       " [0.031242492, 0.05921017],\n",
       " [0.08224442, 0.77389693],\n",
       " [0.43025425, 0.7679605],\n",
       " [0.07438286, 0.5801463],\n",
       " [0.7566455, 0.9793204],\n",
       " [0.16375102, 0.90008545],\n",
       " [0.4092692, 0.9000187],\n",
       " [0.09233897, 0.6312505],\n",
       " [0.4153766, 0.94247],\n",
       " [0.29061845, 0.93318945],\n",
       " [0.31486297, 0.8535564],\n",
       " [0.092020415, 0.56431806],\n",
       " [0.18138605, 0.8223739],\n",
       " [0.048638012, 0.70767],\n",
       " [0.8062752, 0.9612379],\n",
       " [0.2503977, 0.850383],\n",
       " [0.32463494, 0.8454668],\n",
       " [0.14635907, 0.7997432],\n",
       " [0.17097549, 0.78763235],\n",
       " [0.0642029, 0.55087644],\n",
       " [0.74051285, 0.9630868],\n",
       " [0.09903165, 0.6833163],\n",
       " [0.13691285, 0.706503],\n",
       " [0.22652714, 0.8194769],\n",
       " [0.20569311, 0.85767496],\n",
       " [0.7483672, 0.9844683],\n",
       " [0.041498825, 0.5640848],\n",
       " [0.39514014, 0.9026952],\n",
       " [0.480897, 0.9827603],\n",
       " [0.07285797, 0.6485498],\n",
       " [0.20786776, 0.82422537],\n",
       " [0.01485424, 0.25651342],\n",
       " [0.2025019, 0.59288186],\n",
       " [0.0033353276, 0.26250568],\n",
       " [0.29919058, 0.9013916],\n",
       " [0.07137818, 0.6908896],\n",
       " [0.83392644, 0.98200315],\n",
       " [0.061263584, 0.7770808],\n",
       " [0.38912562, 0.6095656],\n",
       " [0.05214842, 0.43188652],\n",
       " [0.27748892, 0.93118006],\n",
       " [0.09370576, 0.5998617],\n",
       " [0.12822345, 0.67229724],\n",
       " [0.08629306, 0.89816827],\n",
       " [0.14740822, 0.72922593],\n",
       " [0.08990486, 0.7309653],\n",
       " [0.10625931, 0.56890285],\n",
       " [0.17644523, 0.8603489],\n",
       " [0.48727864, 0.9656884],\n",
       " [0.30201825, 0.82453644],\n",
       " [0.015073209, 0.09129981],\n",
       " [0.0051329983, 0.095568426],\n",
       " [0.09663815, 0.77809715],\n",
       " [0.32981843, 0.8930398],\n",
       " [0.09283387, 0.48758078],\n",
       " [0.05163609, 0.3639527],\n",
       " [0.058880925, 0.33310476],\n",
       " [0.111178, 0.7397774],\n",
       " [0.19517398, 0.9428683],\n",
       " [0.06845132, 0.4873729],\n",
       " [0.8797748, 0.9827063],\n",
       " [0.029703215, 0.13616973],\n",
       " [0.53371257, 0.8555311],\n",
       " [0.11306243, 0.44864658],\n",
       " [0.1928266, 0.78401047],\n",
       " [0.2187715, 0.8162058],\n",
       " [0.10823074, 0.7333335],\n",
       " [0.15416512, 0.8175265],\n",
       " [0.083070934, 0.9035265],\n",
       " [0.08936877, 0.8312566],\n",
       " [0.013879635, 0.117001675],\n",
       " [0.12131685, 0.78883106],\n",
       " [0.64125884, 0.9410252],\n",
       " [0.52279276, 0.9433537],\n",
       " [0.04736331, 0.6144049],\n",
       " [0.22257127, 0.778601],\n",
       " [0.42872515, 0.9291852],\n",
       " [0.08062779, 0.74870986],\n",
       " [0.18979059, 0.8505311],\n",
       " [0.058907215, 0.48059127],\n",
       " [0.0042402237, 0.27613044],\n",
       " [0.33126432, 0.77510995],\n",
       " [0.09356004, 0.6507282],\n",
       " [0.043374464, 0.5931901],\n",
       " [0.04146576, 0.33164805],\n",
       " [0.5653545, 0.94076073],\n",
       " [0.07122993, 0.4896904],\n",
       " [0.012859195, 0.12256704],\n",
       " [0.1649781, 0.8436909],\n",
       " [0.42511633, 0.92979884],\n",
       " [0.14853296, 0.7926516],\n",
       " [0.0041023735, 0.10809994],\n",
       " [0.11446453, 0.8406296],\n",
       " [0.23003225, 0.87884325],\n",
       " [0.17369895, 0.85386187],\n",
       " [0.23192689, 0.71604276],\n",
       " [0.1178918, 0.7083407],\n",
       " [0.014726276, 0.15954012],\n",
       " [0.14697883, 0.6100361],\n",
       " [0.7529856, 0.67979324],\n",
       " [0.54418665, 0.97073925],\n",
       " [0.48500142, 0.958704],\n",
       " [0.24714217, 0.8289048],\n",
       " [0.21190746, 0.90053],\n",
       " [0.17878765, 0.7563755],\n",
       " [0.05091897, 0.59865385],\n",
       " [0.021333905, 0.19834998],\n",
       " [0.114037186, 0.35007396],\n",
       " [0.059085995, 0.39164704],\n",
       " [0.030577369, 0.31413493],\n",
       " [0.20175205, 0.7329327],\n",
       " [0.05042821, 0.42461762],\n",
       " [0.06865509, 0.43506598],\n",
       " [0.13125981, 0.72944856],\n",
       " [0.39410788, 0.9100914],\n",
       " [0.6469208, 0.9685929],\n",
       " [0.11232112, 0.8610724],\n",
       " [0.12386287, 0.5675516],\n",
       " [0.24002646, 0.72127795],\n",
       " [0.20059705, 0.75177586],\n",
       " [0.1115563, 0.81912446],\n",
       " [0.110468484, 0.88868934],\n",
       " [0.041959364, 0.7571854],\n",
       " [0.21655002, 0.77333397],\n",
       " [0.13861701, 0.72647434],\n",
       " [0.100658365, 0.8276177],\n",
       " [0.01235591, 0.2776785],\n",
       " [0.13504276, 0.7316834],\n",
       " [0.068439916, 0.5952007],\n",
       " [0.047350857, 0.7595757],\n",
       " [0.38475236, 0.91163945],\n",
       " [0.049558435, 0.6521236],\n",
       " [0.17400306, 0.52111787],\n",
       " [0.25365824, 0.78361017],\n",
       " [0.22617684, 0.9114246],\n",
       " [0.2749409, 0.82583773],\n",
       " [0.11938862, 0.6010876],\n",
       " [0.18730357, 0.7947268],\n",
       " [0.18813875, 0.863363],\n",
       " [0.121077195, 0.70884633],\n",
       " [0.023400735, 0.78806305],\n",
       " [0.1382589, 0.7212566],\n",
       " [0.14858271, 0.8723392],\n",
       " [0.15278517, 0.7909397],\n",
       " [0.7850188, 0.972073],\n",
       " [0.05069326, 0.6181833],\n",
       " [0.15101823, 0.624032],\n",
       " [0.25988725, 0.9245607],\n",
       " [0.091603644, 0.5814683],\n",
       " [0.094554916, 0.50189453],\n",
       " [0.51151717, 0.96088946],\n",
       " [0.083924256, 0.68337554],\n",
       " [0.063041694, 0.46465468],\n",
       " [0.16543539, 0.89835876],\n",
       " [0.17070213, 0.7027823],\n",
       " [0.11874588, 0.76723593],\n",
       " [0.04911735, 0.33453065],\n",
       " [0.07358777, 0.86756015],\n",
       " [0.42936707, 0.93271726],\n",
       " [0.22587287, 0.8229876],\n",
       " [0.77887815, 0.9654555],\n",
       " [0.20505773, 0.84946615],\n",
       " [0.32397348, 0.9364146],\n",
       " [0.05780335, 0.79935217],\n",
       " [0.14258012, 0.8845791],\n",
       " [0.25289416, 0.8781411],\n",
       " [0.38453636, 0.8986649],\n",
       " [0.28520712, 0.81251377],\n",
       " [0.18499556, 0.6887402],\n",
       " [0.53275985, 0.92885137],\n",
       " [0.06942682, 0.5718991],\n",
       " [0.009822071, 0.278547],\n",
       " [0.52830714, 0.95843625],\n",
       " [0.14782427, 0.60607624],\n",
       " [0.5753233, 0.83224],\n",
       " [0.015521606, 0.23582838],\n",
       " [0.04163024, 0.85871005],\n",
       " [0.27723432, 0.6998197],\n",
       " [0.090839714, 0.88649726],\n",
       " [0.21431167, 0.80979365],\n",
       " [0.20291683, 0.9508628],\n",
       " [0.2614395, 0.81676304],\n",
       " [0.5823527, 0.9321402],\n",
       " [0.6866218, 0.9831699],\n",
       " [0.05863495, 0.5978045],\n",
       " [0.042095467, 0.6842316],\n",
       " [0.005937947, 0.066829674],\n",
       " [0.02416656, 0.17362194],\n",
       " [0.04530179, 0.70672554],\n",
       " [0.1326382, 0.79488236],\n",
       " [0.35422155, 0.88434196],\n",
       " [0.40379217, 0.84119177],\n",
       " [0.3304825, 0.91196996],\n",
       " [0.059502963, 0.7647868],\n",
       " [0.120008424, 0.85782194],\n",
       " [0.08430173, 0.7587003],\n",
       " [0.05371081, 0.65823865],\n",
       " [0.049319588, 0.17738156],\n",
       " [0.2848858, 0.9433252],\n",
       " [0.14655378, 0.87660265],\n",
       " [0.08684378, 0.8148351],\n",
       " [0.029187314, 0.53015137],\n",
       " [0.14539044, 0.6358264],\n",
       " [0.54536307, 0.88503754],\n",
       " [0.42857745, 0.94094324],\n",
       " [0.04778405, 0.21265356],\n",
       " [0.45421687, 0.96064395],\n",
       " [0.8211544, 0.96617436],\n",
       " [0.9387877, 0.9881792],\n",
       " [0.1298198, 0.6790206],\n",
       " [0.10887753, 0.8325814],\n",
       " [0.09177349, 0.820438],\n",
       " [0.019914033, 0.47765628],\n",
       " [0.18686026, 0.7118386],\n",
       " [0.18032351, 0.6956919],\n",
       " [0.08030143, 0.80753434],\n",
       " [0.09235782, 0.7419421],\n",
       " [0.45284316, 0.96811086],\n",
       " [0.08979959, 0.8579996],\n",
       " [0.10513008, 0.5262973],\n",
       " [0.044717174, 0.6299907],\n",
       " [0.362886, 0.87992865],\n",
       " [0.013037218, 0.44647187],\n",
       " [0.10362964, 0.40155935],\n",
       " [0.011953402, 0.6054981],\n",
       " [0.32759172, 0.7426266],\n",
       " [0.6255019, 0.97732747],\n",
       " [0.08434785, 0.6117542],\n",
       " [0.038126033, 0.6661807],\n",
       " [0.2451425, 0.7872416],\n",
       " [0.08663018, 0.6677349],\n",
       " [0.27971715, 0.8527131],\n",
       " [0.5117874, 0.73157257],\n",
       " [0.31038508, 0.89286256],\n",
       " [0.566532, 0.9356462],\n",
       " [0.32555735, 0.9143794],\n",
       " [0.19368492, 0.91162837],\n",
       " [0.30555123, 0.8704373],\n",
       " [0.1466547, 0.663893],\n",
       " [0.1976963, 0.74258715],\n",
       " [0.20065676, 0.8278326],\n",
       " [0.04273044, 0.32949424],\n",
       " [0.18030438, 0.7520695],\n",
       " [0.08216434, 0.72972447],\n",
       " [0.32216772, 0.8329295],\n",
       " [0.40900797, 0.94885886],\n",
       " [0.14729758, 0.8493164],\n",
       " [0.06689937, 0.22560443],\n",
       " [0.0999377, 0.8048144],\n",
       " [0.100554064, 0.5100489],\n",
       " [0.10305925, 0.7017447],\n",
       " [0.13153067, 0.83765787],\n",
       " [0.12511273, 0.7115685],\n",
       " [0.097098045, 0.9294738],\n",
       " [0.15954554, 0.4841926],\n",
       " [0.020503014, 0.18469837],\n",
       " [0.2784969, 0.96505886],\n",
       " [0.6652651, 0.9878891],\n",
       " [0.048852324, 0.16870496],\n",
       " [0.0554275, 0.44301763],\n",
       " [0.66030705, 0.9417776],\n",
       " [0.14867525, 0.8613357],\n",
       " [0.5025135, 0.92511815],\n",
       " [0.08155216, 0.77529067],\n",
       " [0.09090386, 0.8699554],\n",
       " [0.37889898, 0.82661116],\n",
       " [0.008843752, 0.068322055],\n",
       " [0.08974935, 0.6208524],\n",
       " [0.023648018, 0.8030282],\n",
       " [0.2456454, 0.8979286],\n",
       " [0.26777026, 0.89636445],\n",
       " [0.108429156, 0.78968006],\n",
       " [0.11870957, 0.58076346],\n",
       " [0.033996824, 0.82919127],\n",
       " [0.05500514, 0.72787493],\n",
       " [0.021427965, 0.72666115],\n",
       " [0.092531145, 0.8020246],\n",
       " [0.3316014, 0.94215184],\n",
       " [0.043188445, 0.7333775],\n",
       " [0.2776154, 0.840747],\n",
       " [0.045861818, 0.25083396],\n",
       " [0.047333598, 0.62203914],\n",
       " [0.14238504, 0.5624657],\n",
       " [0.07872541, 0.7624341],\n",
       " [0.14241292, 0.82008976],\n",
       " [0.16318999, 0.83570665],\n",
       " [0.65905625, 0.97424036],\n",
       " [0.23411782, 0.663349],\n",
       " [0.101906195, 0.36324874],\n",
       " [0.2974357, 0.84375966],\n",
       " [0.060507074, 0.4676784],\n",
       " [0.06862601, 0.7430162],\n",
       " [0.541279, 0.9396112],\n",
       " [0.17912617, 0.47447062],\n",
       " [0.5153905, 0.9185164],\n",
       " [0.77987504, 0.9941186],\n",
       " [0.13494097, 0.80814326],\n",
       " [0.37069845, 0.8323983],\n",
       " [0.01050842, 0.27949238],\n",
       " [0.037809435, 0.36987174],\n",
       " [0.09299269, 0.58012193],\n",
       " [0.20490505, 0.8381497],\n",
       " [0.09510483, 0.85817367],\n",
       " [0.226275, 0.7146458],\n",
       " [0.117529064, 0.8837632],\n",
       " [0.111502334, 0.57068473],\n",
       " [0.019569151, 0.34251332],\n",
       " [0.6538342, 0.9475914],\n",
       " [0.27134702, 0.7966537],\n",
       " [0.023028063, 0.13883151],\n",
       " [0.076750405, 0.8268935],\n",
       " [0.215872, 0.68274057],\n",
       " [0.055359535, 0.90508807],\n",
       " [0.11485477, 0.7884291],\n",
       " [0.17810547, 0.8639688],\n",
       " [0.042002685, 0.6578497],\n",
       " [0.14943391, 0.82610416],\n",
       " [0.19010578, 0.85507923],\n",
       " [0.7424098, 0.9286181],\n",
       " [0.028872345, 0.4103587],\n",
       " [0.02346268, 0.3974393],\n",
       " [0.12925632, 0.8976086],\n",
       " [0.23071848, 0.9406488],\n",
       " [0.5910071, 0.90490395],\n",
       " [0.24079232, 0.82363427],\n",
       " [0.3820373, 0.55224687],\n",
       " [0.034975704, 0.40129402],\n",
       " [0.2799808, 0.82834136],\n",
       " [0.191962, 0.8119046],\n",
       " [0.25179064, 0.8411747],\n",
       " [0.04995024, 0.7545615],\n",
       " [0.10098944, 0.69596785],\n",
       " [0.16620754, 0.38685834],\n",
       " [0.08794411, 0.7217302],\n",
       " [0.2113805, 0.96927243],\n",
       " [0.009029427, 0.2657792],\n",
       " [0.111704506, 0.78032357],\n",
       " [0.55548793, 0.95118535],\n",
       " [0.41042373, 0.7973202],\n",
       " [0.39616778, 0.9392745],\n",
       " [0.102812275, 0.73564005],\n",
       " [0.05418932, 0.46959835],\n",
       " [0.043902066, 0.49946997],\n",
       " [0.20245713, 0.8942696],\n",
       " [0.31172556, 0.9438352],\n",
       " [0.074201114, 0.5908838],\n",
       " [0.054024395, 0.48775464],\n",
       " [0.16330643, 0.727835],\n",
       " [0.2631461, 0.7156511],\n",
       " [0.03421654, 0.5208797],\n",
       " [0.29891902, 0.8398884],\n",
       " [0.015370096, 0.07163446],\n",
       " [0.05369407, 0.63230014],\n",
       " [0.15482704, 0.59944355],\n",
       " [0.1976561, 0.7237131],\n",
       " [0.12726487, 0.49178773],\n",
       " [0.07823828, 0.41317666],\n",
       " [0.10618814, 0.71443933],\n",
       " [0.6372323, 0.95966923],\n",
       " [0.624379, 0.96255994],\n",
       " [0.068509825, 0.5363487],\n",
       " [0.122748375, 0.684072],\n",
       " [0.30565387, 0.9334283],\n",
       " [0.11496707, 0.76808035],\n",
       " [0.13936597, 0.75213253],\n",
       " [0.073749706, 0.6227573],\n",
       " [0.0796126, 0.4738245],\n",
       " [0.14265642, 0.90353423],\n",
       " [0.18982719, 0.7157554],\n",
       " [0.053968094, 0.66702664],\n",
       " [0.45672575, 0.83733165],\n",
       " [0.66339254, 0.731982],\n",
       " [0.13629855, 0.6462056],\n",
       " [0.04905518, 0.6311061],\n",
       " [0.12729102, 0.7907838],\n",
       " [0.10287431, 0.7229422],\n",
       " [0.15031132, 0.9257574],\n",
       " [0.14243904, 0.6605323],\n",
       " [0.14270918, 0.68342674],\n",
       " [0.0595755, 0.6874225],\n",
       " [0.32293236, 0.85469055],\n",
       " [0.6473754, 0.95456064],\n",
       " [0.5342285, 0.98494685],\n",
       " [0.3546771, 0.95944685],\n",
       " [0.5946956, 0.9649254],\n",
       " [0.18672143, 0.87828636],\n",
       " [0.073101275, 0.2627049],\n",
       " [0.12719578, 0.7079418],\n",
       " [0.15914421, 0.57085556],\n",
       " [0.8900856, 0.9948277],\n",
       " [0.2633974, 0.7117843],\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = model1.predict_proba(X_train)\n",
    "pred2 = model2.predict_proba(X_train)\n",
    "probs = []\n",
    "for i in range(len(pred1)):\n",
    "    probs.append([pred1[i][1], pred2[i][1]])\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d831923",
   "metadata": {},
   "source": [
    "## When testing it on the central site, the results are very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41bcc13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.96      0.92     64040\n",
      "         1.0       0.76      0.54      0.63     15960\n",
      "\n",
      "    accuracy                           0.87     80000\n",
      "   macro avg       0.83      0.75      0.78     80000\n",
      "weighted avg       0.87      0.87      0.87     80000\n",
      "\n",
      "TP, FP, TN, FN: \n",
      "(8621, 2762, 61278, 7339)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0).fit(probs, y_train)\n",
    "lr_pred = lr.predict(probs)\n",
    "print(classification_report(y_train, lr_pred))\n",
    "print(\"TP, FP, TN, FN: \")\n",
    "print(perf_measure(y_train, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397db620",
   "metadata": {},
   "source": [
    "# But when it is tested on test set, we see that it is overfitting and actually hurting overall effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48037056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.93      0.72     20032\n",
      "         1.0       0.83      0.35      0.49     19968\n",
      "\n",
      "    accuracy                           0.64     40000\n",
      "   macro avg       0.71      0.64      0.60     40000\n",
      "weighted avg       0.71      0.64      0.60     40000\n",
      "\n",
      "TP, FP, TN, FN: \n",
      "(6947, 1450, 18582, 13021)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(probs, y_train)\n",
    "lr_pred = lr.predict(prob_test)\n",
    "print(classification_report(y_test, lr_pred))\n",
    "print(\"TP, FP, TN, FN: \")\n",
    "print(perf_measure(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d758b",
   "metadata": {},
   "source": [
    "# Saving model and then passing around sites, not parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18b6aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_model('model1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "750586bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewzhang/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:104: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:16:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.26      0.40     20032\n",
      "         1.0       0.56      0.95      0.71     19968\n",
      "\n",
      "    accuracy                           0.61     40000\n",
      "   macro avg       0.70      0.61      0.55     40000\n",
      "weighted avg       0.70      0.61      0.55     40000\n",
      "\n",
      "TP, FP, TN, FN: \n",
      "(19013, 14780, 5252, 955)\n"
     ]
    }
   ],
   "source": [
    "incr_model = XGBClassifier(use_label_encoder=False)\n",
    "incr_model.fit(xt2, yt2, xgb_model='model1.model')\n",
    "incr_model.save_model('incr_model.model')\n",
    "incr_pred = incr_model.predict(X_test)\n",
    "print(classification_report(y_test, incr_pred))\n",
    "print(\"TP, FP, TN, FN: \")\n",
    "print(perf_measure(y_test, incr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b986e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewzhang/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:104: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:16:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incr_model2 = XGBClassifier(use_label_encoder=False)\n",
    "incr_model2.fit(X_train, y_train, xgb_model='incr_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb6f90e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.95      0.71     20032\n",
      "         1.0       0.84      0.27      0.41     19968\n",
      "\n",
      "    accuracy                           0.61     40000\n",
      "   macro avg       0.71      0.61      0.56     40000\n",
      "weighted avg       0.71      0.61      0.56     40000\n",
      "\n",
      "TP, FP, TN, FN: \n",
      "(5379, 988, 19044, 14589)\n"
     ]
    }
   ],
   "source": [
    "incr_pred2 = incr_model2.predict(X_test)\n",
    "print(classification_report(y_test, incr_pred2))\n",
    "print(\"TP, FP, TN, FN: \")\n",
    "print(perf_measure(y_test, incr_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6097b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
